# Task Description 4

In this exercise, you will implement a custom K-means clustering algorithm from scratch that supports multiple distance metrics. K-means is one of the most popular clustering algorithms used to partition data into K distinct, non-overlapping clusters.

## Tasks:

### Exercise 4.1
Implement a `MyKMeans` class in `ex_04_my_kmeans_.py` with the following functionality:
   - Support for different distance metrics: Euclidean, Manhattan, and Dynamic Time Warping (DTW). For DTW you can use the `dtaidistance` library, look at the documentation for a fast implementation of the `dtw` function.
   - Support for different initialization methods: random and k-means++
   - Ability to handle both 2D data (standard feature vectors) and 3D data (time series with multiple features)
Your implementation should include:
   - `__init__` method to initialize parameters
   - `fit` method to train the model on input data
   - `predict` method to assign new data points to clusters
   - `fit_predict` method to combine fitting and prediction
   - Proper handling of convergence and tracking of inertia (sum of distances to nearest centroid)

Make sure your implementation can:
   - Accept both NumPy arrays and Pandas DataFrames as input
   - Show progress during training using tqdm
   - Handle edge cases like empty clusters

#### Note:
For k-means++ you can use the following links: (https://www.geeksforgeeks.org/ml-k-means-algorithm/, https://en.wikipedia.org/wiki/K-means%2B%2B)

1. Initialization Step:

   - Choose the first cluster center randomly from the data points.
   - For each remaining cluster center select the next center based on the probability that is proportional to the square of the distance between the data point and the closest selected center.

2. Clustering Step:

   - After selecting the initial centers KMeans++ performs clustering the same way as KMeans:
   - Assign each data point to the nearest cluster center.
   - Recalculate cluster centers by finding the average of all points in each cluster.
   - Repeat the steps until the cluster centers do not change or a fixed number of iterations is reached.

### Exercise 4.2 in `ex_04_clustering.ipynb`
Use the elbow method to find the optimal number of clusters for the raw time series data using your implementation of the kmeans algorithm (MyKMeans).

1. Calculate the distortion (inertia) for different values of k (1 to 10)
2. Plot the results to identify the "elbow point" with matplotlib where adding more clusters produces diminishing returns
3. This will help determine the most appropriate number of clusters for our welding data


### Exercise 4.3 in `ex_04_clustering.ipynb`

#### Based on the elbow curve above, explain your choice of k:

1. What does the shape of the elbow curve tell you about the underlying data structure?
2. Why did you select this specific number of clusters?
   - Consider the plot and the elbow method to justify your choice
   - How might this choice affect the interpretability of the resulting clusters?

#### KMeans Implementation
1. Run K-means with your selected k value using both:
   - Euclidean distance
   - DTW distance
2. Compare the clustering results between the two distance metrics


### Exercise 4.4 in `ex_04_clustering.ipynb`

Plot the mean time series (centroids) for each cluster to visualize and understand the patterns.

Remember that our welding data has both current and voltage measurements over time (shape: n_samples, sequence_length, features). For each cluster:
1. Plot the average current pattern
2. Plot the average voltage pattern
3. Look for distinctive characteristics in each cluster that might relate to welding quality

This visualization will help identify what makes each cluster unique in terms of temporal patterns.


### Exercise 4.5 in `ex_04_clustering.ipynb`
Now we'll use the engineered features extracted in Exercise 3 instead of raw time series data. Therefore, you can use the euclidean distance metric.

1. Load your extracted features from exercise 3 
2. Split them into data and labels
3. Scale the data for better clustering performance
4. Apply the elbow method again to determine the optimal number of clusters for the feature-based approach
5. Compare this result with the clustering of raw time series data. Consider why the optimal k might differ between the two approaches:
   - Do engineered features represent the data differently?
   - Which approach might better capture the relevant patterns for quality assessment?

### Exercise 4.6 in `ex_04_clustering.ipynb`

Visualize the clustering results using interactive 3D plots with Plotly.

1. Use PCA to reduce the dimensionality of our feature space to 3 components
2. Create two visualizations:
   - Points colored by assigned cluster
   - Same points colored by actual quality labels
3. Include the explained variance for each principal component in the axis labels
4. Save the figures to the plot_path

This visualization will help us understand how well our clustering approach aligns with the known quality designations.

#### Note:
- You can use the following links to find more information about the PCA:
   - https://en.wikipedia.org/wiki/Principal_component_analysis
   - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html

### Exercise 4.7 in `ex_04_clustering.ipynb`

Analyze the clustering results in depth and discuss their implications:

1. Cluster separation:
   - How well do clusters separate different quality levels?
   - What is the Adjusted Rand Index between clusters and quality labels?
   - Are there clusters that predominantly contain good or bad welds?

2. Feature importance:
   - Which features seem most important for distinguishing clusters?
   - How does the PCA visualization help us understand the data structure?

3. Process insights:
   - What insights could these clusters provide for improving the welding process?
   - Could certain clusters identify specific types of welding issues?

4. Limitations:
   - What are the limitations of using clustering for quality assessment?
   - How might the approach be improved in future iterations?


## Expected Output

Your clustering algorithm should produce:
- A set of k centroids
- Cluster assignments for each data point
- The inertia value for the clustering solution 

## Testing Your Implementation
To run the tests, use:
```bash
pytest tests/test_ex_04_my_kmeans.py -v
```

