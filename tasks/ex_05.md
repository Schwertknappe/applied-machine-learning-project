# Task Description 5
In this exercise, you will explore different classification models to predict welding quality. You will implement a pipeline that includes hyperparameter tuning using cross-validation and evaluate model performance on a separate test set. This exercise will utilize both the engineered features from Exercise 3 and the raw time-series data.

## Objectives:
1. Select and implement four different classification algorithms from scikit-learn.
2. Design and execute a hyperparameter search strategy with cross-validation for each classifier.
3. Train models using both engineered features and raw time-series data.
4. Evaluate model performance using appropriate metrics and justify your choice of metrics.
5. Test the final tuned models on a held-out test dataset.


## Tasks:
### Exercise 5.1: Classifier Selection
- Choose four distinct classification algorithms from the `scikit-learn` library suitable for this task.
- Briefly justify your choice of these four classifiers in your notebook/report. Consider their strengths, weaknesses, and suitability for the welding quality prediction problem.


### Exercise 5.2: Hyperparameter Search Setup
- For each selected classifier:
  - Define a hyperparameter grid or distribution to search over.
  - Use a `scikit-learn` Pipeline to chain any preprocessing steps (e.g., scaling) with your classifier to prevent data leakage and streamline your workflow.
  - Implement a hyperparameter search using `GridSearchCV` or `RandomizedSearchCV` with cross-validation.
  - Perform this search twice:
    1. Using the engineered features extracted in Exercise 3.
    2. Using the raw time-series data (you might need to flatten or adapt it for certain classifiers).
- Clearly define your training, validation (implicit in CV), and test data splits. Ensure the test set is held out and used only for final evaluation.
- Address class imbalance in the training data. You can explore techniques like setting the `class_weight` parameter in your classifiers or using resampling methods like SMOTE from the `imbalanced-learn` library within your pipeline. You can pick one of the techniques and explain why you chose it.

#### Note
- You can use the `scikit-learn` documentation to find apriopiate hyperparameters, which could influence the performance of a selected model.
- Look for [Oversampling and undersampling in data analysis](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) and [imbalanced-learn](https://imbalanced-learn.org/stable/) to understand the different techniques to handle class imbalance.

### Exercise 5.3: Evaluation Metrics
- Select at least three appropriate evaluation metrics for this classification task (e.g., accuracy, precision, recall, F1-score, ROC AUC).
    - Justify why these metrics are suitable for evaluating welding quality prediction, considering potential class imbalances or specific costs associated with misclassifications.
- For all subsequent steps compute these metrics for all models.


### Exercise 5.4: Model Training and Tuning
- Train each of the four classifiers using the hyperparameter search strategy defined in Exercise 5.2 and determin the best set of hyperparameters on the validation scores (do not use the test set yet).
- Document the best hyperparameters found for each classifier and feature set (engineered vs. raw).
- Use the best set of hyperparameters of each classifier and train the model three times with different random seeds. Now use the test set to evaluate the model and save the result metrics in a csv file. The goal is to compute the mean and standard deviation over the different random seeds, so that we can have a robust metric for the model performance.  


## Implementation
- use python files or jupyter notebooks to implement Task 5. Each file should start with "ex_05".
- Ensure your code is reproducible by setting all relevant random seeds (e.g., for data splits, model initialization, and resampling techniques).
- Include a `requirements.txt` or updated  `pyproject.toml` file to document your project's dependencies.
- Save the result metrics as average and standard deviation over the three random seeds in seperate csv files.

  
## Notes:
- Use the `get_welding_data()` function from Exercise 1 to load data. (If you did not finish part 1 we provided the code for the loading of data)
- Remember to preprocess data appropriately for each type of classifier and feature set (e.g., scaling for feature-based models).
- For raw time-series data, consider techniques like flattening the time series or using classifiers that can inherently handle sequential data (though the latter might be more advanced than scikit-learn's standard offerings).
- Pay attention to data leakage, especially when performing cross-validation and test set evaluation.
